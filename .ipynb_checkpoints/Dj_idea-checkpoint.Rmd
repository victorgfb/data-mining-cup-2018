---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    notebook_metadata_filter: jupytext_formats
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.3.1
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
# %autosave 0
```

```{python}
import pandas as pd
import numpy as np
import datetime
from sklearn.linear_model import LogisticRegression
import re
from sklearn import preprocessing

regex = r"([0-9]{4}-[0-9]{2})-([0-9]{2})"
```

```{python}
train = pd.read_csv('train.csv', sep='|')
items = pd.read_csv('items.csv', sep='|')

items = items.fillna("None")
train = train.fillna("None")
```

```{python}
month = []
for i in train["date"]:
    month.append(re.search(regex, i).group(1))
```

```{python}
train.insert(1,"month",month)
```

```{python}
train
```

```{python}
def insert_column (data,fields,fields_name):
    index = len(data.columns)-1
    for i in range(len(fields_name)):
        data.insert(index + i,fields_name[i],fields[fields_name[i]],True)
    return data
```

```{python}
items
```

```{python}
from copy import deepcopy
new_data = deepcopy(items)
```

```{python}
units = train.sort_values(by=['units'])['units'].reset_index()['units']
```

```{python}
units[units.index > 134000]
```

```{python}
import matplotlib.pyplot as plt 
plt.plot(units) 
   
plt.title('Units graph!') 
  
# function to show the plot 
plt.show() 
```

```{python}
train = train[train['units'] < 20]
```

```{python}
max(train['units'])
```

```{python}
train
```

```{python}
x = train.groupby(['pid','size','month']).tail(1).sort_values(by=['pid']).drop(['units'], axis=1)
```

```{python}
grouped = train.groupby(['pid','size','month']).sum()
```

```{python}
grouped
```

```{python}
grouped.index = x.index
```

```{python}
#x['units'] = grouped['units']
```

```{python}
x = x.sort_values(by=['pid','month'])
```

```{python}
x
```

```{python}
data = pd.merge(items.drop(['stock'], axis=1), x, on=['pid','size'])
```

```{python}
data['units'] = grouped['units'].values
```

```{python}
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X = LabelEncoder()

data['mainCategory'] = labelencoder_X.fit_transform(data['mainCategory'])
```

```{python}
#onehotencoder.fit_transform(data)
```

```{python}
categories = pd.get_dummies(data['category'])
mainCategories = pd.get_dummies(data['mainCategory'])
subCategories = pd.get_dummies(data['subCategory'])
color = pd.get_dummies(data['color'])
brand = pd.get_dummies(data['brand'])
```

```{python}
# categories.columns = ['cat-2', 'cat-7', 'cat-10', 'cat-16', 'cat-18', 'cat-24', 'cat-30', 'cat-33', 'cat-36', 'cat-37']

mainCategories.columns = ['mcat-1', 'mcat-9', 'mcat-15']

# subCategories.columns = [ 'subcat-3.0',    'subcat-4.0',    'subcat-5.0',    'subcat-6.0',    'subcat-8.0',   'subcat-11.0',   'subcat-12.0',   'subcat-13.0',   'subcat-14.0',
#          'subcat-16.0',   'subcat-17.0',   'subcat-19.0',   'subcat-20.0',   'subcat-21.0',   'subcat-22.0',   'subcat-23.0',   'subcat-25.0',   'subcat-26.0',
#          'subcat-27.0',   'subcat-28.0',   'subcat-29.0',   'subcat-31.0',   'subcat-32.0',   'subcat-34.0',   'subcat-35.0',   'subcat-38.0',   'subcat-39.0',
#          'subcat-40.0',   'subcat-41.0',   'subcat-42.0',   'subcat-43.0',   'subcat-44.0', 'subcat-None']
```

```{python}
# data = insert_column(data,categories, categories.columns)
data = insert_column(data,mainCategories, mainCategories.columns)
# data = insert_column(data,subCategories, subCategories.columns)
# data = insert_column(data, color, color.columns)
# data = insert_column(data, brand, brand.columns)
```

```{python}
new_sum = data[data['month'] < '2018-01'].groupby(['pid','size']).sum()['units']
```

```{python}
data = data.drop(['category'], axis=1)
data = data.drop(['mainCategory'], axis=1)
data = data.drop(['subCategory'], axis=1)
data = data.drop(['color'], axis=1)
#data = data.drop(['pid'], axis=1)
data = data.drop(['brand'], axis=1)
#data = data.drop(['size'], axis=1)
#data = data.drop(['releaseDate'], axis=1)
data = data.drop(['mcat-1'], axis=1)
```

```{python}
from datetime import datetime
data['target'] = data['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').day)
data = data.drop(['date'], axis=1)
```

```{python}
new_test = data[data['month'] == '2018-01']
```

```{python}
from datetime import datetime

d2 = datetime.strptime('2018-01-01','%Y-%m-%d')
new_test['releaseDate'] = new_test['releaseDate'].apply(lambda x: (d2 -datetime.strptime(x,'%Y-%m-%d')).days)
new_test = new_test[new_test['releaseDate'] >= 0]
```

```{python}
new_data = pd.merge( new_sum,new_test, on=['pid','size'])
```

```{python}
new_data
```

```{python}
t = train.sort_values(by=['pid','size','date']).drop(['units'], axis=1)
```

```{python}
t = t[t['month'] < '2018-01']
```

```{python}
t['date'] = pd.to_datetime(t['date'], format='%Y-%m-%d')
```

```{python}
t['diff'] = t.groupby(['pid','size'])['date'].diff().apply(lambda x: x.days)
```

```{python}
t
```

```{python}
releases = items[['pid', 'size', 'releaseDate']] 
```

```{python}
a = pd.merge(t, releases, on=['pid','size'])
```

```{python}
a
```

```{python}
a['releaseDate'] = pd.to_datetime(a['releaseDate'], format='%Y-%m-%d')
```

```{python}
untilRelease = (a['date'] - a['releaseDate']).apply(lambda x: x.days)
```

```{python}
untilRelease
```

```{python}
import math
for i in range(len(a)):
    if(not (math.isnan(a.iloc[i]['diff']))):
        untilRelease[i] = a.iloc[i]['diff']
```

```{python}
a = a.drop(['releaseDate'], axis=1)
a['diff'] = untilRelease
```

```{python}
# import math
# def untilRelease(x):
#     aux = datetime.strptime(items[(items['pid'] == x['pid'])][items['size'] == x['size']]['releaseDate'],'%Y-%m-%d')
#     print(aux)
#     if(math.isnan(x['diff'])):
#         x['di']  = (datetime.strptime(x['date'],'%Y-%m-%d') - aux).days
        
# t.apply(untilRelease)
```

```{python}
new_data
```

```{python}
a.groupby(['pid','size']).mean()
```

```{python}
new_data = pd.merge(a.groupby(['pid','size']).mean(), new_data, on=['pid','size'])
```

```{python}
len(new_data)
```

```{python}
#new_test.columns = ['rrp', 'month', 'mcat-9', 'mcat-15', 'stock', 'target']
new_data = new_data.drop(['month'], axis=1)
new_data = new_data.drop(['pid'], axis=1)
new_data = new_data.drop(['size'], axis=1)
y = new_data.iloc[:,-1]
X = new_data.iloc[:,0:-1]
```

```{python}
from sklearn.model_selection import train_test_split
 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
```

```{python}
# data_test = data[data['month'] == '2018-01']
# data_test = data_test.drop(['month'], axis=1)
# y_test = data_test.iloc[:,-1]
# x_test = data_test.iloc[:,0:-1]

# data_train = data[data['month'] != '2018-01']
# data_train = data_train.drop(['month'], axis=1)
# y_train = data_train.iloc[:,-1]
# x_train = data_train.iloc[:,0:-1]

# data = data.drop(['month'], axis=1)
# y_data = data.iloc[:,-1]
# x_data = data.iloc[:,0:-1]
```

```{python}
# scaler = preprocessing.StandardScaler()
# x_data = scaler.fit_transform(x_data)
```

```{python}
new_data
```

```{python}
from sklearn.ensemble import RandomForestRegressor

regr = RandomForestRegressor(max_depth=2)
regr.fit(X_train, y_train)
regr.score(X_test, y_test)
```

```{python}
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)
regressor.score(X_test, y_test)
```

```{python}
# from sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict
# from sklearn.ensemble import RandomForestRegressor

# gsc = GridSearchCV( estimator=RandomForestRegressor(),param_grid={'max_depth': range(2,7),'n_estimators': (10, 50, 100, 500, 750, 1000),},cv=10, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)
# grid_result = gsc.fit(X_train, y_train)
# best_params = grid_result.best_params_
```

```{python}
# rfr = RandomForestRegressor(max_depth=best_params["max_depth"], n_estimators=best_params["n_estimators"],random_state=False, verbose=False)
# rfr.fit(X_train, y_train)
# predictions = rfr.predict(X_test)
# #cores = cross_val_score(rfr, x_data, y_data, cv=10, scoring='neg_mean_absolute_error')
```

```{python}
# grid_result.best_params_
```

```{python}
from sklearn.metrics import r2_score
r2_score(y_test, predictions)
```

```{python}
from sklearn.svm import SVR
clf = SVR(kernel='rbf', epsilon=0.1)
clf.fit(X_train, y_train)
predictions = clf.predict(X_test)
```

```{python}
from sklearn.metrics import r2_score
r2_score(y_test, predictions)
```

```{python}
from sklearn.ensemble import AdaBoostRegressor
clf = AdaBoostRegressor(learning_rate=0.01, n_estimators=1000)
clf.fit(X_train, y_train)
predictions = clf.predict(X_test)
```

```{python}
from sklearn.metrics import r2_score
r2_score(y_test, predictions)
```

```{python}
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeRegressor

clf = DecisionTreeRegressor(max_depth=3)
clf.fit(X_train, y_train)
predictions = clf.predict(X_test)
```

```{python}
from sklearn.metrics import r2_score
r2_score(y_test, predictions)
```

```{python}
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor

rng = np.random.RandomState(1)
regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=3), n_estimators=1000, random_state=rng)
regr_2.fit(X_train, y_train)
predictions = regr_2.predict(X_test)
```

```{python}
from sklearn.metrics import r2_score
r2_score(y_test, predictions)
```

```{python}

```
